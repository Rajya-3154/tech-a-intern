{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Analyzing Covid-19 Data with PySpark",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajya-3154/tech-a-intern/blob/main/Analyzing_Covid_19_Data_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'our-world-in-data-covid19-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1753898%2F3408825%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240627%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240627T134418Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6819842856fec20b6ec0a74f7c4472a07c5f4c88095e9eb650be4d7304386876e07c2393a2242ff4716febd76e1158848dcc1ae86307f97b5cae2902e049f2cd717aac7457bb76e28fb4285f5ef8d6f5fe3c4b3832feb4385c6fe7927bd60f3a928ce452fb51df64ebaf6a82bb0165f0528044ffe949a6d0d1d041b6fff58267f16700d4567a5199703cc1205651e581a37e2e83edbb228a641dd07ec1485c71ed4aec07cbe14419cc0b23c198a5c94ff13bfefa86cae563ae51712ae91372639e68dceddd03f94dfa9f945c43ba66db24b891dfe864a905e6a3b56cbbc14f0274a5d164bd6aa95886f9d58253e9ea3d863851df7acd7dac3fd51800b30ab22f'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "S9lQjdRh8L9v"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='start'></a>\n",
        "\n",
        "# Analyzing Covid-19 Data with PySpark\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "NFl_hJRO8L9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sec0'></a>\n",
        "\n",
        "## 0. Introduction\n",
        "\n",
        "The goal of this project is the loading, preprocessing and exploratory data analysis of SARS-Cov-2 data on a global scale using [PySpark](https://spark.apache.org/docs/latest/api/python/), Apache Spark's Python API. The [dataset](https://github.com/owid/covid-19-data/blob/master/public/data/README.md) was curated and is maintained by [Our World in Data](https://ourworldindata.org/coronavirus) (OWiD). Typically, Spark is utilized when dealing with much larger datasets than the one seen here. In fact, all the tasks performed below could be performed using Pandas, in a somewhat cleaner and more familiar fashion. Nonetheless, the purpose of this notebook is not the analysis itself (especially since Covid-19 data have been extensively analyzed before), but the introduction of the reader to PySpark. In this sense, the dataset is more of a workhorse serving an educational purpose, so Pandas will be used only when absolutely necessary."
      ],
      "metadata": {
        "id": "6jekYQYO8L9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sec1'></a>\n",
        "\n",
        "## 1. Data Loading & Overview\n",
        "\n",
        "The present notebook was written in a local environment using Jupyter Lab. As always, in case any of the libraries used in the present notebook is not installed in the environment where the reader intends to run it, please ensure their proper installation beforehand. The following command downloads the dataset from the corresponding GitHub repository."
      ],
      "metadata": {
        "id": "I5v886So8L91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install pyspark\n",
        "\n",
        "file = '../input/our-world-in-data-covid19-dataset/owid-covid-data.csv'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:28:03.350828Z",
          "iopub.execute_input": "2022-04-10T13:28:03.351319Z",
          "iopub.status.idle": "2022-04-10T13:29:00.680865Z",
          "shell.execute_reply.started": "2022-04-10T13:28:03.351233Z",
          "shell.execute_reply": "2022-04-10T13:29:00.679977Z"
        },
        "trusted": true,
        "id": "S6sndmga8L91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After initializing a PySpark session, the data are transformed into a PySpark DataFrame."
      ],
      "metadata": {
        "id": "JLaPYnKc8L91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Covid Data Mining\").config('spark.sql.debug.maxToStringFields', 2000).getOrCreate()\n",
        "full_df = spark.read.csv(file, header=True, inferSchema=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:00.682393Z",
          "iopub.execute_input": "2022-04-10T13:29:00.682652Z",
          "iopub.status.idle": "2022-04-10T13:29:13.640895Z",
          "shell.execute_reply.started": "2022-04-10T13:29:00.682614Z",
          "shell.execute_reply": "2022-04-10T13:29:13.639884Z"
        },
        "trusted": true,
        "id": "Y1qbqwby8L91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first identify the total number of samples, as well as the number of each sample's features."
      ],
      "metadata": {
        "id": "qLqh5XoE8L92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The total number of samples is {full_df.count()}, with each sample corresponding to {len(full_df.columns)} features.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:13.643064Z",
          "iopub.execute_input": "2022-04-10T13:29:13.643409Z",
          "iopub.status.idle": "2022-04-10T13:29:14.790848Z",
          "shell.execute_reply.started": "2022-04-10T13:29:13.643361Z",
          "shell.execute_reply": "2022-04-10T13:29:14.790008Z"
        },
        "trusted": true,
        "id": "2TDcrnOh8L92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to identify each feature, as well as its type, `full_df.dtypes` can be used. Alternatively, they are available as part of the Schema's information via the following:"
      ],
      "metadata": {
        "id": "6uGiYEGk8L92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_df.printSchema()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:14.793433Z",
          "iopub.execute_input": "2022-04-10T13:29:14.793731Z",
          "iopub.status.idle": "2022-04-10T13:29:14.803898Z",
          "shell.execute_reply.started": "2022-04-10T13:29:14.793693Z",
          "shell.execute_reply": "2022-04-10T13:29:14.802021Z"
        },
        "trusted": true,
        "id": "XUfnwB2c8L92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the features are `double` types, i.e. numerical data. However, nominal features are also present:\n",
        "\n",
        "- `iso_code`: a string corresponding to each country's code.\n",
        "- `location`: a string corresponding to each location's name.\n",
        "- `continent`: a string corresponding to the continent where the location belongs.\n",
        "- `tests_units`: a string corresponding to the units used in each location in order to count the number of tests (more details below).\n",
        "\n",
        "There's also the `date` feature, the type of which is `string`, however it will be properly transformed into a datetime object in what follows. The following command gives some examples for each of these features."
      ],
      "metadata": {
        "id": "ZCTr2Fda8L93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_df.select(\"iso_code\",\"location\",\"continent\",\"date\",\"tests_units\").show(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:14.80562Z",
          "iopub.execute_input": "2022-04-10T13:29:14.806244Z",
          "iopub.status.idle": "2022-04-10T13:29:15.14868Z",
          "shell.execute_reply.started": "2022-04-10T13:29:14.806198Z",
          "shell.execute_reply": "2022-04-10T13:29:15.147789Z"
        },
        "trusted": true,
        "id": "uolVOes68L93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the `tests_units` column in the above command contains several null values, indicating that many of them may appear in the present dataset. We may import `functions` from `pyspark.sql`, which allows us to perform aggregations (among other things) and count the exact number of `null` values for each feature. The results can be stored in a dictionary, which can then be sorted to present the results in an orderly fashion."
      ],
      "metadata": {
        "id": "_nk7dW0L8L93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "miss_vals = full_df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in full_df.columns]).collect()[0].asDict()\n",
        "miss_vals = dict(sorted(miss_vals.items(), reverse=True, key=lambda item: item[1]))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_records([miss_vals])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:15.149866Z",
          "iopub.execute_input": "2022-04-10T13:29:15.150177Z",
          "iopub.status.idle": "2022-04-10T13:29:18.813744Z",
          "shell.execute_reply.started": "2022-04-10T13:29:15.150138Z",
          "shell.execute_reply": "2022-04-10T13:29:18.812928Z"
        },
        "trusted": true,
        "id": "1DNi5nh18L93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the above Pandas is utilized only for the visualization of the dataframe, so it's not considered as \"cheating\"."
      ],
      "metadata": {
        "id": "acyy5dk48L93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sec2'></a>\n",
        "\n",
        "## 2. Data Preprocessing\n",
        "\n",
        "Taking the above into consideration, the next step is the construction of a filtered version of the full DataFrame, which includes only the January - February 2021 time period. The reason for this is that - up to the day that this notebook is written - the OWiD dataset on Covid-19 is still being expanded. Consequently, any conclusions that may be drawn as part of the present analysis on the full dataset may be altered in the future, when more data become available and a reader attempts to run the notebook as it is. Before doing that, we make sure that the `date` feature is transformed into a `date` type object."
      ],
      "metadata": {
        "id": "sWUlUg6G8L93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_df = full_df.withColumn('date',F.to_date(F.unix_timestamp(F.col('date'), 'yyyy-MM-dd').cast(\"timestamp\")))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:18.815247Z",
          "iopub.execute_input": "2022-04-10T13:29:18.816021Z",
          "iopub.status.idle": "2022-04-10T13:29:18.868823Z",
          "shell.execute_reply.started": "2022-04-10T13:29:18.815973Z",
          "shell.execute_reply": "2022-04-10T13:29:18.867847Z"
        },
        "trusted": true,
        "id": "ZlxMXE1y8L93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, `full_df` is filtered in order to keep only the data for the aforementioned two months."
      ],
      "metadata": {
        "id": "6qnDHKfG8L93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates = (\"2021-01-01\", \"2021-02-28\")\n",
        "df = full_df.where(F.col('date').between(*dates))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:18.87004Z",
          "iopub.execute_input": "2022-04-10T13:29:18.870334Z",
          "iopub.status.idle": "2022-04-10T13:29:18.943059Z",
          "shell.execute_reply.started": "2022-04-10T13:29:18.870295Z",
          "shell.execute_reply": "2022-04-10T13:29:18.942068Z"
        },
        "trusted": true,
        "id": "sNPrDw3I8L94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For completeness, we perform for this filtered version the basic tasks that were performed for the full DataFrame:"
      ],
      "metadata": {
        "id": "fS6-pJsm8L94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The total number of samples is {df.count()}, with each sample corresponding to {len(df.columns)} features.\")\n",
        "\n",
        "miss_vals = df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
        "miss_vals = dict(sorted(miss_vals.items(), reverse=True, key=lambda item: item[1]))\n",
        "\n",
        "pd.DataFrame.from_records([miss_vals])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:18.944298Z",
          "iopub.execute_input": "2022-04-10T13:29:18.950717Z",
          "iopub.status.idle": "2022-04-10T13:29:23.286597Z",
          "shell.execute_reply.started": "2022-04-10T13:29:18.950657Z",
          "shell.execute_reply": "2022-04-10T13:29:23.285827Z"
        },
        "trusted": true,
        "id": "DFFoLxyU8L94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sec2.1'></a>\n",
        "\n",
        "### 2.1. Handling Missing Values"
      ],
      "metadata": {
        "id": "ToU7reso8L95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='zht2.1'></a>\n",
        "\n",
        "Even in this filtered version, there's a sizeable number of null values present. Before investigating how to deal with them, it's important that we understand the reason why they're missing. As far as the `continent` feature is concerned, the following command sheds light into the reason why it contains null values."
      ],
      "metadata": {
        "tags": [],
        "id": "WI_mbxTF8L95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(\"continent\").select(\"iso_code\",\"continent\",\"location\").show(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:23.289867Z",
          "iopub.execute_input": "2022-04-10T13:29:23.290415Z",
          "iopub.status.idle": "2022-04-10T13:29:24.382128Z",
          "shell.execute_reply.started": "2022-04-10T13:29:23.290372Z",
          "shell.execute_reply": "2022-04-10T13:29:24.381272Z"
        },
        "trusted": true,
        "id": "cPV4u53Y8L95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, OWiD have performed a series of aggregations based on criteria such as income, or general aggregations (for example on the continent level). Since they may prove to be useful later on, there is no reason to discard them. The null values can simply be set equal to the `'OWID'` value, in order to be able to invoke them later on if we need to."
      ],
      "metadata": {
        "id": "vEjz7kdG8L95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna({'continent':'OWID'})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:24.383431Z",
          "iopub.execute_input": "2022-04-10T13:29:24.384005Z",
          "iopub.status.idle": "2022-04-10T13:29:24.474208Z",
          "shell.execute_reply.started": "2022-04-10T13:29:24.383947Z",
          "shell.execute_reply": "2022-04-10T13:29:24.473236Z"
        },
        "trusted": true,
        "id": "eK2Vk3rC8L95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another column which corresponds to a nominal feature with missing values is `tests_units`. The distinct values that this feature assumes are:"
      ],
      "metadata": {
        "id": "oMIaVL-L8L95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"tests_units\").distinct().show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:24.475397Z",
          "iopub.execute_input": "2022-04-10T13:29:24.475687Z",
          "iopub.status.idle": "2022-04-10T13:29:25.585878Z",
          "shell.execute_reply.started": "2022-04-10T13:29:24.475642Z",
          "shell.execute_reply": "2022-04-10T13:29:25.584857Z"
        },
        "trusted": true,
        "id": "Vlra3JBO8L95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, `tests_units` is simply a variable that indicates how each country/location reports on the performed tests. For example, in the case of `people tested`, the reported number of total tests is expected to be lower compared to the same report in the case of `tests performed`, since one person can be tested more than once during the same day. This implies that the missing values are due to some countries/locations not providing the relevant information on how they count the total number of daily tests. Of course, this is not a reason to discard the relevant data, therefore the missing values will be replaced by the string `'no info'`."
      ],
      "metadata": {
        "id": "CKEAJfIA8L95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna({'tests_units':'no info'})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:25.587078Z",
          "iopub.execute_input": "2022-04-10T13:29:25.587369Z",
          "iopub.status.idle": "2022-04-10T13:29:25.664047Z",
          "shell.execute_reply.started": "2022-04-10T13:29:25.58733Z",
          "shell.execute_reply": "2022-04-10T13:29:25.663051Z"
        },
        "trusted": true,
        "id": "39I_I3rm8L96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving on to the quantitative features, most missing values are due to the fact that the relevant data were either not available during the studied time period for some locations, or were simply equal to zero. For example, there are 10272 missing values in the `new_vaccinations` column, which are either due to the fact that vaccines were not available in some locations, or due to the fact that these locations reported no vaccinations for specific dates. The best approach in this case is replacing all these values with 0. In the few cases where the missing values are not due to any of these two reasons, but due to wrong reports, bugs, or other reasons, we expect to find it out during their analysis and especially their visualization. In this case, we will be able to re-handle them or discard them completely."
      ],
      "metadata": {
        "id": "wcZPEpGE8L96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna(0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:25.665195Z",
          "iopub.execute_input": "2022-04-10T13:29:25.665478Z",
          "iopub.status.idle": "2022-04-10T13:29:25.830135Z",
          "shell.execute_reply.started": "2022-04-10T13:29:25.665439Z",
          "shell.execute_reply": "2022-04-10T13:29:25.829122Z"
        },
        "trusted": true,
        "id": "frZYOL_n8L96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following confirms that there are no missing values left in the dataset."
      ],
      "metadata": {
        "id": "krRSCcpc8L96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "miss_vals = df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
        "if any(list(miss_vals.values())) != 0:\n",
        "    print(\"There are still missing values in the DataFrame.\")\n",
        "else:\n",
        "    print(\"All missing values have been taken care of.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:25.83243Z",
          "iopub.execute_input": "2022-04-10T13:29:25.832766Z",
          "iopub.status.idle": "2022-04-10T13:29:27.320536Z",
          "shell.execute_reply.started": "2022-04-10T13:29:25.832727Z",
          "shell.execute_reply": "2022-04-10T13:29:27.319503Z"
        },
        "trusted": true,
        "id": "HoI9GMDo8L97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sec2.2'></a>\n",
        "\n",
        "### 2.2. Outlier Detection\n",
        "\n",
        "Having discussed the case of missing values, perhaps it's a good idea to also discuss the case of outliers. Typically, the identification of outliers requires further analysis, such as visualizations, since it is not a trivial matter (in fact, more often than not it's a case of a supervised learning problem on its own). Furthermore, there are several types of outliers, such as global outliers or context-based outliers (i.e. points that are outliers only given a specific condition or context), which means that dealing with outliers in a universal manner is ill-advised. Nonetheless, if one chooses to do so, a systematic way to deal with outliers is based on [interquartile range methods](https://en.wikipedia.org/wiki/Interquartile_range). The interquartile range, $R$, is defined as\n",
        "\n",
        "$$ R = Q_3 - Q_1 $$\n",
        "\n",
        "where $Q_i$ is the $i$-th quartile. Every point for which the studied feature has a value higher than $Q_3 + \\alpha R$ or lower than $Q_1 - \\alpha R$ is classified as an outlier for this specific feature, where $\\alpha$ is a scalar that defines a \"decision boundary\" in units of $R$. This is essentially how [Box plots](https://en.wikipedia.org/wiki/Box_plot) are constructed, where $R$ corresponds to the Box's height and $\\alpha R$ is equal to the whiskers' length. One very common choice for $\\alpha$ is $\\alpha = 1.5$.\n",
        "\n",
        "Based on these, one can define a function that identifies all outliers with respect to specific features."
      ],
      "metadata": {
        "id": "MfSGQ2UM8L97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OutlierDetector(dataframe, features, alpha=1.5):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dataframe (pyspark.sql.dataframe.DataFrame):\n",
        "            the DataFrame hosting the data\n",
        "        features (string or List):\n",
        "            List of features (columns) for which we wish to identify outliers.\n",
        "            If set equal to 'all', outliers are identified with respect to all features.\n",
        "        alpha (double):\n",
        "            The parameter that defines the decision boundary (see markdown above)\n",
        "    \"\"\"\n",
        "    feat_types = dict(dataframe.dtypes)\n",
        "    if features == 'all':\n",
        "        features = dataframe.columns\n",
        "\n",
        "    outliers_cols = []\n",
        "\n",
        "    for feat in features:\n",
        "        # We only care for quantitative features\n",
        "        if feat_types[feat] == 'double':\n",
        "            Q1, Q3 = dataframe.approxQuantile(feat, [0.25, 0.75], 0)\n",
        "            R = Q3 - Q1\n",
        "            lower_bound = Q1 - (R * alpha)\n",
        "            upper_bound = Q3 + (R * alpha)\n",
        "\n",
        "            # In this way we construct a query, which can be matched to a DataFrame column, thus returning a new\n",
        "            # column where every point that corresponds to an Outlier has a boolean value set to True\n",
        "            outliers_cols.append(F.when(~F.col(feat).between(lower_bound, upper_bound), True).alias(feat + '_outlier'))\n",
        "\n",
        "    # Sample points that do not correspond to outliers correspond to a False value for the new column\n",
        "    outlier_df = dataframe.select(*outliers_cols)\n",
        "    outlier_df = outlier_df.fillna(False)\n",
        "    return outlier_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:27.322425Z",
          "iopub.execute_input": "2022-04-10T13:29:27.32291Z",
          "iopub.status.idle": "2022-04-10T13:29:27.33456Z",
          "shell.execute_reply.started": "2022-04-10T13:29:27.322861Z",
          "shell.execute_reply": "2022-04-10T13:29:27.333477Z"
        },
        "trusted": true,
        "id": "6hEtmV4k8L97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, we can check if any of 5 random DataFrame rows correspond to outliers with respect to the `new_cases` feature:"
      ],
      "metadata": {
        "id": "etj-Iezv8L98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_df = OutlierDetector(dataframe=df, features=['new_cases'], alpha=1.5)\n",
        "out_df.show(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:27.336412Z",
          "iopub.execute_input": "2022-04-10T13:29:27.336925Z",
          "iopub.status.idle": "2022-04-10T13:29:28.751548Z",
          "shell.execute_reply.started": "2022-04-10T13:29:27.336882Z",
          "shell.execute_reply": "2022-04-10T13:29:28.750645Z"
        },
        "trusted": true,
        "id": "vKyFuoaP8L98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sec2.3'></a>\n",
        "\n",
        "#### 2.3. Duplicate Entries\n",
        "\n",
        "Before proceeding to the exploratory data analysis, the final step of the preprocessing phase is to locate possible duplicate entries and discard the duplicates. When speaking of duplicates we do not actually refer to a whole row, but rather the combined entries of the `date` **and** `location` columns. A duplicate entry on both of these features would imply that the location has provided more than one daily report on a given date. The following command shows that no duplicates exist in the filtered DataFrame, however, even if they did, they could be removed using ```df = df.dropDuplicates(['location','date'])```."
      ],
      "metadata": {
        "id": "PkUK6JWb8L-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if df.count() != df.select(['location','date']).distinct().count():\n",
        "    print(\"There are duplicate entries present in the DataFrame.\")\n",
        "else:\n",
        "    print(\"Either there are no duplicate entries present in the DataFrame, or all of them have already been removed).\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:28.752705Z",
          "iopub.execute_input": "2022-04-10T13:29:28.753042Z",
          "iopub.status.idle": "2022-04-10T13:29:30.627761Z",
          "shell.execute_reply.started": "2022-04-10T13:29:28.752996Z",
          "shell.execute_reply": "2022-04-10T13:29:30.626693Z"
        },
        "trusted": true,
        "id": "lvb6NJ7R8L-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sec3'></a>\n",
        "\n",
        "## 3. Exploratory Data Analysis\n",
        "\n",
        "Before diving into the EDA, we import some libraries and also present some helper functions and commands that will be utilized further down the road for visualizations."
      ],
      "metadata": {
        "id": "nzUg7deQ8L-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap, TwoSlopeNorm\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "def CustomCmap(from_rgb,to_rgb):\n",
        "\n",
        "    # from color r,g,b\n",
        "    r1,g1,b1 = from_rgb\n",
        "\n",
        "    # to color r,g,b\n",
        "    r2,g2,b2 = to_rgb\n",
        "\n",
        "    cdict = {'red': ((0, r1, r1),\n",
        "                   (1, r2, r2)),\n",
        "           'green': ((0, g1, g1),\n",
        "                    (1, g2, g2)),\n",
        "           'blue': ((0, b1, b1),\n",
        "                   (1, b2, b2))}\n",
        "\n",
        "    cmap = LinearSegmentedColormap('custom_cmap', cdict)\n",
        "    return cmap\n",
        "\n",
        "mycmap = CustomCmap([1.0, 1.0, 1.0], [72/255, 99/255, 147/255])\n",
        "mycmap_r = CustomCmap([72/255, 99/255, 147/255], [1.0, 1.0, 1.0])\n",
        "\n",
        "mycol = (72/255, 99/255, 147/255)\n",
        "mycomplcol = (129/255, 143/255, 163/255)\n",
        "othercol1 = (135/255, 121/255, 215/255)\n",
        "othercol2 = (57/255, 119/255, 171/255)\n",
        "othercol3 = (68/255, 81/255, 91/255)\n",
        "othercol4 = (73/255, 149/255, 139/255)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:30.629003Z",
          "iopub.execute_input": "2022-04-10T13:29:30.629301Z",
          "iopub.status.idle": "2022-04-10T13:29:31.576108Z",
          "shell.execute_reply.started": "2022-04-10T13:29:30.62926Z",
          "shell.execute_reply": "2022-04-10T13:29:31.575234Z"
        },
        "trusted": true,
        "id": "STTJ2amJ8L-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Evolution of top countries with respect to mortality**\n",
        "\n",
        "Herein, the mortality rate is calculated as the total number of deaths divided by each location's population (another common definition is the total number of deaths by Covid divided by the total number of Covid cases). For this purpose, a column named `mortality` is constructed. Using this column, we identify the top 10 countries in terms of mortality rates, for every day of the studied time interval."
      ],
      "metadata": {
        "id": "QkUkYjJ_8L-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates_frame = df.select(\"date\").distinct().orderBy('date').collect()\n",
        "dates_list = [str(dates_frame[x][0]) for x in range(len(dates_frame))]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:31.577326Z",
          "iopub.execute_input": "2022-04-10T13:29:31.577556Z",
          "iopub.status.idle": "2022-04-10T13:29:32.83568Z",
          "shell.execute_reply.started": "2022-04-10T13:29:31.577529Z",
          "shell.execute_reply": "2022-04-10T13:29:32.834366Z"
        },
        "trusted": true,
        "id": "X9wcHWP78L-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_for_mort = df.filter(F.col('population') != 0.0).withColumn(\"mortality\", F.col(\"total_deaths\")/F.col(\"population\"))\n",
        "\n",
        "for i, this_day in enumerate(dates_list):\n",
        "    this_day_top_10 = df_for_mort.filter(F.col('date') == this_day).orderBy(\"mortality\", ascending=False).select([\"location\",\"mortality\"]).take(10)\n",
        "    if i == 0:\n",
        "        ct_list = [(this_day_top_10[x][0],this_day_top_10[x][1]) for x in range(10)]\n",
        "        print(\"During \"+this_day+\", the top 10 countries with the highest mortality rate were:\")\n",
        "        for country, instance in ct_list:\n",
        "            print(f\"▶ {country}, with mortality rate {100*instance:.2f}%.\")\n",
        "        new_set = set(ct_list[x][0] for x in range(10))\n",
        "    elif i == len(dates_list)-1:\n",
        "        ct_list = [(this_day_top_10[x][0],this_day_top_10[x][1]) for x in range(10)]\n",
        "        print(\"During \"+this_day+\", the top 10 countries with the highest mortality rate were:\")\n",
        "        for country, instance in ct_list:\n",
        "            print(f\"▶ {country}, with mortality rate {100*instance:.2f}%.\")\n",
        "    else:\n",
        "        new_set = set(this_day_top_10[x][0] for x in range(10))\n",
        "        if new_set != old_set:\n",
        "            left_out = old_set-new_set\n",
        "            new_additions = new_set-old_set\n",
        "            print(\"This was the top ten until \"+this_day+\", when \"+\", \".join(str(s) for s in new_additions)+\" joined the list, replacing \"+\", \".join(str(s) for s in left_out)+\".\")\n",
        "    new_set, old_set = set(), new_set"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:29:32.836837Z",
          "iopub.execute_input": "2022-04-10T13:29:32.837131Z",
          "iopub.status.idle": "2022-04-10T13:30:16.703861Z",
          "shell.execute_reply.started": "2022-04-10T13:29:32.837092Z",
          "shell.execute_reply": "2022-04-10T13:30:16.703122Z"
        },
        "trusted": true,
        "id": "VWSNbQvP8L-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A better way of approaching this is by developing a series of lollipop charts that depict the evolution of the top 10 countries with respect to the mortality rate. The python code used to extract each lollipop chart image can be found in the [Appendix](#appendix) and all images were merged into a .gif file using Adobe Photoshop. The result can be seen below:\n",
        "\n",
        "<center> <img src=\"https://srigas.me/github/covid_mortality_rate.gif\" width=420> </center>\n",
        "\n",
        "The lollipop chart provides the information that the code above provided. In addition to that, it presents the exact changes in ratings between the top 10 countries."
      ],
      "metadata": {
        "id": "BoUN6RLM8L-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Evolution of top countries with respect to total cases per million**\n",
        "\n",
        "The same procedure can be performed for the number of total cases per million. We choose to normalize the total number of cases in this way in order to be able to compare locations with different populations."
      ],
      "metadata": {
        "id": "aIjIClhK8L-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, this_day in enumerate(dates_list):\n",
        "    this_day_top_10 = df.filter(F.col('date') == this_day).orderBy(\"total_cases_per_million\", ascending=False).select([\"location\",\"total_cases_per_million\"]).take(10)\n",
        "    if i == 0:\n",
        "        ct_list = [(this_day_top_10[x][0],this_day_top_10[x][1]) for x in range(10)]\n",
        "        print(\"During \"+this_day+\", the top 10 countries with the highest number of total cases per million were:\")\n",
        "        for country, instance in ct_list:\n",
        "            print(f\"▶ {country}, with {instance} total cases per million.\")\n",
        "        new_set = set(ct_list[x][0] for x in range(10))\n",
        "    elif i == len(dates_list)-1:\n",
        "        ct_list = [(this_day_top_10[x][0],this_day_top_10[x][1]) for x in range(10)]\n",
        "        print(\"During \"+this_day+\", the top 10 countries with the highest number of total cases per million were:\")\n",
        "        for country, instance in ct_list:\n",
        "            print(f\"▶ {country}, with {instance} total cases per million.\")\n",
        "    else:\n",
        "        new_set = set(this_day_top_10[x][0] for x in range(10))\n",
        "        if new_set != old_set:\n",
        "            left_out = old_set-new_set\n",
        "            new_additions = new_set-old_set\n",
        "            print(\"This was the top ten until \"+this_day+\", when \"+\", \".join(str(s) for s in new_additions)+\" joined the list, replacing \"+\", \".join(str(s) for s in left_out)+\".\")\n",
        "    new_set, old_set = set(), new_set"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:16.705221Z",
          "iopub.execute_input": "2022-04-10T13:30:16.705432Z",
          "iopub.status.idle": "2022-04-10T13:30:54.861263Z",
          "shell.execute_reply.started": "2022-04-10T13:30:16.705407Z",
          "shell.execute_reply": "2022-04-10T13:30:54.860648Z"
        },
        "trusted": true,
        "id": "2Dt7Xfnr8L-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corresponding .gif image can be seen below.\n",
        "\n",
        "<center> <img src=\"https://srigas.me/github/total_cases_covid_lollipop.gif\" width=420> </center>"
      ],
      "metadata": {
        "id": "sQfdOyIy8L-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Hospitalized Patients and ICU Admissions**\n",
        "\n",
        "Moving on, we study the `hosp_patients` and `icu_patients` features by visualizing the corresponding timeseries for the total number of hospitalized and ICU patients on a global scale."
      ],
      "metadata": {
        "tags": [],
        "id": "jPhhdaeb8L-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_ord = df.orderBy(\"date\", ascending=True).groupBy(\"date\")\n",
        "\n",
        "hosps = dt_ord.agg(F.sum(\"hosp_patients\")).collect()\n",
        "hosps = [hosps[i][1] for i in range(len(hosps))]\n",
        "\n",
        "icus = dt_ord.agg(F.sum(\"icu_patients\")).collect()\n",
        "icus = [icus[i][1] for i in range(len(icus))]\n",
        "\n",
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "alt_dts_list = [dt.replace('2021-', '') for dt in dates_list]\n",
        "tick_marks = np.arange(len(alt_dts_list))\n",
        "\n",
        "fig, [ax1,ax2] = plt.subplots(1, 2, figsize=(14,5))\n",
        "\n",
        "for pat, col, style, ax, where in zip([hosps,icus], [mycol, mycomplcol],\n",
        "                                      ['solid', 'dashed'], [ax1,ax2], ['Normal Beds','ICUs']):\n",
        "    ax.plot(alt_dts_list, pat, linestyle=style, color=col)\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Number of Patients\")\n",
        "    ax.set_title(f\"Daily Number of Patients in {where}\", fontsize=14)\n",
        "    ax.set_xticks(tick_marks[::5])\n",
        "    ax.set_xticklabels(alt_dts_list[::5], rotation=45)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "matplotlib.rc_file_defaults()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:33:35.362198Z",
          "iopub.execute_input": "2022-04-10T13:33:35.362493Z",
          "iopub.status.idle": "2022-04-10T13:33:38.335796Z",
          "shell.execute_reply.started": "2022-04-10T13:33:35.362464Z",
          "shell.execute_reply": "2022-04-10T13:33:38.334885Z"
        },
        "trusted": true,
        "id": "4kcYwOof8L-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It becomes evident that the overall trend is downwards for both hospital and ICU admissions, since both numbers have declined to almost half their initial value by the end of the two-month period under study. It is worth noting that an upwards trend seems to appear near the end of February in the case of ICU patients. Of course, without further information we can't know if it is the beginning of a monotonically increasing trend, or simply a momentary increase, as the one identified between February 15-20. Finally, notice that both diagrams have a similar behavior, which hints at a correlation between the number of hospital patients and the number of ICU patients (which is probably expected). An important difference is that the absolute value of the number of hospital patients is considerably higher compared to the number of ICU admissions, which is reasonable, since the number of milder cases is higher compared to the number of more severe ones."
      ],
      "metadata": {
        "id": "JHJWMWgd8L-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Geographic Heatmap of Total Cases**\n",
        "\n",
        "An interesting visualization is the geographic heatmap, which is a 2D representation of countries world-wide which are colored depending on their intensity as far as a specific feature is concerned. Below, we construct the geographic heatmap for the number of total cases on a global scale. A heatmap image is extracted for each day and afterwards all images are merged into a .gif file. The heatmap is constructed using the geopandas library, as seen below. Note that to do this, we must first download a shapefile (.shp) which is the foundation for the construction of the heatmap and can be found [here](https://www.naturalearthdata.com/downloads/10m-cultural-vectors/)."
      ],
      "metadata": {
        "id": "BZC8M6Mg8L-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, zipfile\n",
        "from io import BytesIO\n",
        "\n",
        "zip_file_url = \"https://srigas.me/kaggle/owid-nb-data.zip\"\n",
        "\n",
        "request = requests.get(zip_file_url)\n",
        "zipDocument = zipfile.ZipFile(BytesIO(request.content))\n",
        "\n",
        "zipDocument.extractall()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:33:51.472623Z",
          "iopub.execute_input": "2022-04-10T13:33:51.47295Z",
          "iopub.status.idle": "2022-04-10T13:33:55.285883Z",
          "shell.execute_reply.started": "2022-04-10T13:33:51.472915Z",
          "shell.execute_reply": "2022-04-10T13:33:55.28458Z"
        },
        "trusted": true,
        "id": "ScWmC2AY8L-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "shapefile = 'countries.shp'\n",
        "geo_df = gpd.read_file(shapefile)[['ADMIN','ADM0_A3','geometry']]\n",
        "geo_df.columns = ['location', 'iso_code', 'geometry']\n",
        "geo_df = geo_df.drop(geo_df.loc[geo_df['location'] == 'Antarctica'].index) # exclude Antarctica"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:33:58.997586Z",
          "iopub.execute_input": "2022-04-10T13:33:58.997894Z",
          "iopub.status.idle": "2022-04-10T13:34:00.243992Z",
          "shell.execute_reply.started": "2022-04-10T13:33:58.997857Z",
          "shell.execute_reply": "2022-04-10T13:34:00.242856Z"
        },
        "trusted": true,
        "id": "YgJkqleB8L-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Initializing the construction of heatmaps for every day.')\n",
        "\n",
        "ct = 0\n",
        "for this_day in dates_list:\n",
        "    # The conversion of the required columns into a Pandas df is necessary to perform the mapping\n",
        "    day_df = df.filter(F.col('date') == this_day).select([\"iso_code\",\"total_cases\"]).toPandas()\n",
        "\n",
        "    merged_df = pd.merge(left=geo_df, right=day_df, how='left', left_on='iso_code', right_on='iso_code')\n",
        "\n",
        "    title = f'Total COVID-19 Cases as of {this_day}'\n",
        "    col = 'total_cases'\n",
        "    vmin, vmax = merged_df[col].min(), merged_df[col].max()\n",
        "    cmap = mycmap\n",
        "    divnorm = TwoSlopeNorm(vcenter=0.08*20365726)\n",
        "\n",
        "    # Create figure and axes for Matplotlib\n",
        "    fig, ax = plt.subplots(1, figsize=(20, 8))\n",
        "\n",
        "    # Remove the axis\n",
        "    ax.axis('off')\n",
        "    merged_df.plot(column=col, ax=ax, edgecolor='1.0', linewidth=1, norm=divnorm, cmap=cmap)\n",
        "\n",
        "    # Add a title\n",
        "    ax.set_title(title, fontdict={'fontsize': '25', 'fontweight': '3'})\n",
        "\n",
        "    # Create colorbar as a legend\n",
        "    sm = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=vmin, vmax=vmax), cmap=cmap)\n",
        "\n",
        "    # Empty array for the data range\n",
        "    sm._A = []\n",
        "\n",
        "    # Add the colorbar to the figure\n",
        "    cbaxes = fig.add_axes([0.15, 0.25, 0.01, 0.4])\n",
        "    cbar = fig.colorbar(sm, cax=cbaxes)\n",
        "    plt.savefig(f'world_map_{this_day}.png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    ct += 1\n",
        "\n",
        "print(f'Process complete. {ct} heatmap(s) were extracted, ready to be converted into a .gif file.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:34:00.246664Z",
          "iopub.execute_input": "2022-04-10T13:34:00.246977Z",
          "iopub.status.idle": "2022-04-10T13:36:40.459083Z",
          "shell.execute_reply.started": "2022-04-10T13:34:00.246941Z",
          "shell.execute_reply": "2022-04-10T13:36:40.458166Z"
        },
        "trusted": true,
        "id": "9nWVHIm_8L-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final .gif can be seen below.\n",
        "\n",
        "<center> <img src=\"https://srigas.me/github/covidgif.gif\"> </center>"
      ],
      "metadata": {
        "id": "uh7Hbz0F8L-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Geographic Correlation of Excess Mortality**\n",
        "\n",
        "Based on the previous visualization it appears that some neighbouring countries are correlated with respect to the total number of cases (for example France and Germany). A reasonable hypothesis is that the same may be true for other features as well, such as the excess mortality.\n",
        "\n",
        "The excess mortality is a feature for which the reports are weekly and not daily. It is equal to the total number of deaths for a specific week minus the mean number of deaths, based on reports from previous years. While it is not a feature directly connected with Covid, it's expected that during a global pandemic the excess mortality can be mainly attributed to this pandemic.\n",
        "\n",
        "In order to investigate the correlation between neighbouring countries, we must first develop a list of dates for which reports on excess mortality are available (for all other dates, the entries are equal to zero due to our preprocessing)."
      ],
      "metadata": {
        "tags": [],
        "id": "0HHV419d8L-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exc_dates_list = df.filter(F.col('excess_mortality') != 0.0).select(['date']).distinct().orderBy('date').collect()\n",
        "exc_dates_list = [str(exc_dates_list[i][0]) for i in range(len(exc_dates_list))]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:36:40.461095Z",
          "iopub.execute_input": "2022-04-10T13:36:40.461417Z",
          "iopub.status.idle": "2022-04-10T13:36:41.354417Z",
          "shell.execute_reply.started": "2022-04-10T13:36:40.461377Z",
          "shell.execute_reply": "2022-04-10T13:36:41.352077Z"
        },
        "trusted": true,
        "id": "FoVNfZP68L-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For brevity, we shall focus our study only on European countries. First, we construct a geographic heatmap of Europe with respect to excess mortality for each date calculated in the previous cell and merge the results into a .gif file, as done previously."
      ],
      "metadata": {
        "id": "CwU9IRBO8L-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Initializing the construction of heatmaps for every day.')\n",
        "\n",
        "ct = 0\n",
        "for this_day in exc_dates_list:\n",
        "    europe_df = df.filter(F.col('date') == this_day).filter(F.col('continent') == 'Europe').filter(F.col('excess_mortality') != 0.0).select([\"iso_code\",\"excess_mortality\"])\n",
        "\n",
        "    geo_eu = pd.merge(left=geo_df, right=europe_df.toPandas(), how='inner', on='iso_code')\n",
        "\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "\n",
        "    col = 'excess_mortality'\n",
        "    cmap = mycmap\n",
        "\n",
        "    vmin, vmax = geo_eu[col].min(), geo_eu[col].max()\n",
        "    sm = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=vmin, vmax=vmax), cmap=cmap)\n",
        "\n",
        "    ax.axis('off')\n",
        "    ax.axis([-13, 44, 33, 72])\n",
        "    geo_eu.plot(column=col, ax=ax, edgecolor='1.0', linewidth=1, norm=None, cmap=cmap)\n",
        "    ax.set_title(f'Excess Mortality in Europe as of {this_day}', fontdict={'fontsize': '14', 'fontweight': '3'})\n",
        "\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=.2)\n",
        "    fig.add_axes(cax)\n",
        "    fig.colorbar(sm, cax=cax)\n",
        "    plt.savefig(f'europe_{this_day}.png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    ct += 1\n",
        "\n",
        "print(f'Process complete. {ct} heatmap(s) were extracted, ready to be converted into a .gif file.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:36:41.355698Z",
          "iopub.execute_input": "2022-04-10T13:36:41.356042Z",
          "iopub.status.idle": "2022-04-10T13:36:53.509419Z",
          "shell.execute_reply.started": "2022-04-10T13:36:41.355999Z",
          "shell.execute_reply": "2022-04-10T13:36:53.508366Z"
        },
        "trusted": true,
        "id": "E67AHTcn8L-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <img src=\"https://srigas.me/github/europe_excess_mortality.gif\"> </center>"
      ],
      "metadata": {
        "id": "jkTstVTP8L-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this visualization, it's safe to assume that there indeed are neighbouring countries for which the excess mortality values appear to be significantly correlated. Germany and Switzerland are an example of one such pair of countries, as they appear to have highs and lows with respect to excess mortality at the same time.\n",
        "\n",
        "In order to produce these results with more mathematical rigor, we need to construct a new PySpark DataFrame including all the reports on excess mortality for each European country that has provided reports on **all** of the previously calculated dates. Countries with even 1 missing value will not be taken into consideration, in order to be able to draw conclusions that are as safe as possible, since the volume of the available data is very small with regards to this feature. Then, using this newly created DataFrame, a Pearson correlation matrix can be constructed, thus revealing not only pairs of correlated countries that share the same geographical borders, but also the exact value of this correlation."
      ],
      "metadata": {
        "id": "zL8nykhC8L-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "european_df = df.filter(F.col('continent') == 'Europe').filter(F.col('excess_mortality') != 0.0)\n",
        "european_cts = european_df.select(['location']).distinct().collect()\n",
        "european_cts = [european_cts[i][0] for i in range(len(european_cts)) if european_df.filter(F.col('location') == european_cts[i][0]).count() == len(exc_dates_list)]\n",
        "print(f'{len(european_cts)} European countries are chosen for this analysis.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:36:53.511764Z",
          "iopub.execute_input": "2022-04-10T13:36:53.51215Z",
          "iopub.status.idle": "2022-04-10T13:37:13.689467Z",
          "shell.execute_reply.started": "2022-04-10T13:36:53.512096Z",
          "shell.execute_reply": "2022-04-10T13:37:13.688505Z"
        },
        "trusted": true,
        "id": "9cvMTjk88L-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "eu_cts_df = european_df.filter(F.col('location') == european_cts[0]).select(['excess_mortality']).withColumnRenamed(\"excess_mortality\", european_cts[0])\n",
        "# required for the proper join of the following DataFrames\n",
        "eu_cts_df = eu_cts_df.withColumn('row_index', row_number().over(Window.partitionBy(F.lit(0)).orderBy(monotonically_increasing_id())))\n",
        "\n",
        "for country in european_cts[1:]:\n",
        "    new_ct_df = european_df.filter(F.col('location') == country).select(['excess_mortality']).withColumnRenamed(\"excess_mortality\", country)\n",
        "    new_ct_df = new_ct_df.withColumn('row_index', row_number().over(Window.partitionBy(F.lit(0)).orderBy(monotonically_increasing_id())))\n",
        "\n",
        "    eu_cts_df = eu_cts_df.join(new_ct_df, on=[\"row_index\"])\n",
        "\n",
        "eu_cts_df = eu_cts_df.drop(\"row_index\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:37:13.690627Z",
          "iopub.execute_input": "2022-04-10T13:37:13.691238Z",
          "iopub.status.idle": "2022-04-10T13:37:17.314665Z",
          "shell.execute_reply.started": "2022-04-10T13:37:13.691205Z",
          "shell.execute_reply": "2022-04-10T13:37:17.314035Z"
        },
        "trusted": true,
        "id": "Sig9LFL28L-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "vector_col = \"corr_features\"\n",
        "assembler = VectorAssembler(inputCols=eu_cts_df.columns, outputCol=vector_col)\n",
        "df_vector = assembler.transform(eu_cts_df).select(vector_col)\n",
        "\n",
        "matrix = Correlation.corr(df_vector, vector_col, 'pearson')\n",
        "cor_np = matrix.collect()[0][matrix.columns[0]].toArray()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:37:17.31608Z",
          "iopub.execute_input": "2022-04-10T13:37:17.316422Z",
          "iopub.status.idle": "2022-04-10T13:37:34.473135Z",
          "shell.execute_reply.started": "2022-04-10T13:37:17.316392Z",
          "shell.execute_reply": "2022-04-10T13:37:34.47245Z"
        },
        "trusted": true,
        "id": "lCQ56FZx8L-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The calculated correlation matrix can be seen in the following heatmap, where only values of Pearson correlation that are higher than 0.8 are depicted (since we are looking for neighbouring countries with high correlation). This is why the lower limit of the colorbar is set to 0.8."
      ],
      "metadata": {
        "id": "rUPXmp4e8L-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(13,10))\n",
        "\n",
        "sns.heatmap(cor_np, linewidths=.5, ax=ax, vmin=0.8, vmax=1, cmap=mycmap,\n",
        "            xticklabels=european_cts, yticklabels=european_cts)\n",
        "ax.set_title('Correlation Matrix for Excess Mortality values per Country', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:37:34.473986Z",
          "iopub.execute_input": "2022-04-10T13:37:34.474188Z",
          "iopub.status.idle": "2022-04-10T13:37:35.354718Z",
          "shell.execute_reply.started": "2022-04-10T13:37:34.474164Z",
          "shell.execute_reply": "2022-04-10T13:37:35.353743Z"
        },
        "trusted": true,
        "id": "grtxRn6N8L-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As noted before, Switzerland and Germany indeed correspond to a pair of highly correlated neighbouring countries with respect to excess mortality. In fact, it appears that in most cases **only** neighbouring countries (for example Belgium and Germany, or Luxemburg and Netherlands) and second neighbours thereof show high values of correlation, with the value of correlation declining significantly as the neighbour index (i.e. how many countries apart two countries are) increases beyond 2. Some additional examples of correlated pairs can be seen in the following table, along with the corresponding Pearson Correlation.\n",
        "\n",
        "| | Pearson Correlation for excess mortality (%) |\n",
        "| :-: | :-: |\n",
        "| **Germany & Switzerland** | 98.29 |\n",
        "| **Germany & Belgium** | 95.44 |\n",
        "| **Lithuania & Poland** | 92.50 |\n",
        "| **Netherlands & Belgium** | 92.30 |\n",
        "| **Czechia & Poland** | 89.08 |\n",
        "| **Greece & Bulgaria** | 86.87 |\n",
        "| **Italy & Slovenia** | 86.23 |\n",
        "\n",
        "For a more extensive list of European countries with high correlation that is not limited only to neighbouring countries, one can run the following snippet of code:\n",
        "\n",
        "```\n",
        "for i in range(len(european_cts)):\n",
        "    for j in range(i+1,len(european_cts)):\n",
        "        corr_val = cor_np[i][j]\n",
        "        if corr_val > 0.8:\n",
        "            print(f'{european_cts[i]} and {european_cts[j]} show a correlation of {100*corr_val:.2f}.')\n",
        "```"
      ],
      "metadata": {
        "id": "swYcb0ub8L-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Reproduction Rate on the Continent Level**\n",
        "\n",
        "Moving on, we continue the study of the pandemic's features on a geographic viewpoint by grouping the countries together into continents. For this purpose, the DataFrame is split into continent-level DataFrames, in order to be able to draw the geographic heatmaps separately. The studied feature is now the daily reproduction rate, corresponding to the heatmap's intensity, with a common scale in order to be able to compare different continents. As usual, a .gif image is constructed using separate heatmap images for each day in the January-February interval.\n",
        "\n",
        "Note that all filterings are performed using PySpark, however the joining of the DataFrames is performed using Pandas because [PySpark cannot recognize the column that corresponds to each location's geometry](https://ncar.github.io/PySpark4Climate/tutorials/pyspark-geo-analysis/geopandas-and-spark/), which is required for the construction of the geographic maps."
      ],
      "metadata": {
        "tags": [],
        "id": "Wt9qC_F78L-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_means = {'AS': [], 'EU' : [], 'NAM' : [], 'SAM' : [], 'OC' : [], 'AF' : []}\n",
        "\n",
        "print('Initializing the construction of heatmaps for every day.')\n",
        "\n",
        "ct = 0\n",
        "for this_day in dates_list:\n",
        "    asia_df = df.filter(F.col('date') == this_day).filter(F.col('continent') == 'Asia').filter(F.col('reproduction_rate') != 0.0).select([\"iso_code\",\"reproduction_rate\"])\n",
        "    europe_df = df.filter(F.col('date') == this_day).filter(F.col('continent') == 'Europe').filter(F.col('reproduction_rate') != 0.0).select([\"iso_code\",\"reproduction_rate\"])\n",
        "    namerica_df = df.filter(F.col('date') == this_day).filter((F.col('continent') == 'North America')).filter(F.col('reproduction_rate') != 0.0).select([\"iso_code\",\"reproduction_rate\"])\n",
        "    samerica_df = df.filter(F.col('date') == this_day).filter((F.col('continent') == 'South America')).filter(F.col('reproduction_rate') != 0.0).select([\"iso_code\",\"reproduction_rate\"])\n",
        "    oceania_df = df.filter(F.col('date') == this_day).filter(F.col('continent') == 'Oceania').filter(F.col('reproduction_rate') != 0.0).select([\"iso_code\",\"reproduction_rate\"])\n",
        "    africa_df = df.filter(F.col('date') == this_day).filter(F.col('continent') == 'Africa').filter(F.col('reproduction_rate') != 0.0).select([\"iso_code\",\"reproduction_rate\"])\n",
        "\n",
        "    daily_means['AS'].append(asia_df.select(F.mean(F.col('reproduction_rate'))).collect()[0][0])\n",
        "    daily_means['EU'].append(europe_df.select(F.mean(F.col('reproduction_rate'))).collect()[0][0])\n",
        "    daily_means['NAM'].append(namerica_df.select(F.mean(F.col('reproduction_rate'))).collect()[0][0])\n",
        "    daily_means['SAM'].append(samerica_df.select(F.mean(F.col('reproduction_rate'))).collect()[0][0])\n",
        "    daily_means['OC'].append(oceania_df.select(F.mean(F.col('reproduction_rate'))).collect()[0][0])\n",
        "    daily_means['AF'].append(africa_df.select(F.mean(F.col('reproduction_rate'))).collect()[0][0])\n",
        "\n",
        "    geo_as = pd.merge(left=geo_df, right=asia_df.toPandas(), how='inner', on='iso_code')\n",
        "    geo_eu = pd.merge(left=geo_df, right=europe_df.toPandas(), how='inner', on='iso_code')\n",
        "    geo_sam = pd.merge(left=geo_df, right=samerica_df.toPandas(), how='inner', on='iso_code')\n",
        "    geo_nam = pd.merge(left=geo_df, right=namerica_df.toPandas(), how='inner', on='iso_code')\n",
        "    geo_oc = pd.merge(left=geo_df, right=oceania_df.toPandas(), how='inner', on='iso_code')\n",
        "    geo_af = pd.merge(left=geo_df, right=africa_df.toPandas(), how='inner', on='iso_code')\n",
        "\n",
        "    fig, axes = plt.subplots(2,3, figsize=(18,14))\n",
        "\n",
        "    col = 'reproduction_rate'\n",
        "    cmap = mycmap\n",
        "\n",
        "    vmin = min(geo_as[col].min(),geo_eu[col].min(),geo_sam[col].min(),geo_nam[col].min(),geo_oc[col].min(),geo_af[col].min())\n",
        "    vmax = max(geo_as[col].max(),geo_eu[col].max(),geo_sam[col].max(),geo_nam[col].max(),geo_oc[col].max(),geo_af[col].max())\n",
        "    sm = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=vmin, vmax=vmax), cmap=cmap)\n",
        "\n",
        "    for ax, data, cont, dims in zip(axes.flat,\n",
        "                               [geo_eu, geo_nam, geo_af, geo_as, geo_sam, geo_oc],\n",
        "                               ['Europe','North America','Africa','Asia','South America','Oceania'],\n",
        "                               [[-13, 44, 33, 72],[-170, -50, 5, 85],[-20, 55, -38, 40],[25, 145, -10, 60],[-85, -32, -58, 15],[110,160,-45,0]]):\n",
        "        title = f'{cont}'\n",
        "        ax.axis('off')\n",
        "        ax.axis(dims)\n",
        "        data.plot(column=col, ax=ax, edgecolor='1.0', linewidth=1, norm=None, cmap=cmap)\n",
        "        ax.set_title(title, fontdict={'fontsize': '18', 'fontweight': '3'})\n",
        "\n",
        "    clb = fig.colorbar(sm, ax=axes.flat, location='bottom', fraction=0.056)\n",
        "    clb.ax.set_title(f'COVID-19 Reproduction Rate as of {this_day}', fontsize=22)\n",
        "    plt.savefig(f'cont_maps_{this_day}.png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    ct += 1\n",
        "\n",
        "print(f'Process complete. {ct} heatmap(s) were extracted, ready to be converted into a .gif file.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.865585Z",
          "iopub.status.idle": "2022-04-10T13:30:59.865922Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.865731Z",
          "shell.execute_reply": "2022-04-10T13:30:59.865747Z"
        },
        "trusted": true,
        "id": "U_sT2W518L-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <img src=\"https://srigas.me/github/daily_covid_reproduction.gif\"> </center>"
      ],
      "metadata": {
        "id": "cXOecuNi8L-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the continental level, it appears that Australia has both the highest and the lowest values for the reproduction rate. Such fluctuations are present in other continents as well, albeit less intense. In general, the reproduction rate appears higher during the last days of February compared to the first days of January, thus indicating an alarming trend of the pandemic. South America is a typical example of this trend, where the reproduction rate appears to stabilize on relatively high values as time passes. On the other hand, there's also the exception of North America (where the fluctuations are uniform for all its countries, thus indicating a high inter-country correlation), where the reproduction rate shows an overall decline during the two studied months.\n",
        "\n",
        "On the country level, Asian countries tend to follow the pattern of neighbouring-countries correlation that was studied above for the case of excess mortality. In fact, in most cases this correlation can be seen between neighbours of order higher than 1, similar to what was observed in North American countries. Interestingly enough, in Europe, while Portugal and Spain show relatively high reproduction rates compared to other countries in the beginning of January, the exact opposite is true by the end of February. On the one hand, this observation can be attributed to the efficient crisis management by the two countries that gained experience after being struck hard by the pandemic during its early days. On the other hand, it can also be attributed to these countries' efficiency in [providing vaccinations coverage](https://apnews.com/article/coronavirus-pandemic-lifestyle-health-europe-portugal-66ad07ae486a8c396cf8a8f72eb8e30f).\n",
        "\n",
        "The .gif image above depicts the reproduction rate on the country level as well, apart from the continent level. In order to focus solely on the continent level, we provide below the timeseries for the mean value of the virus' reproduction rate per continent."
      ],
      "metadata": {
        "id": "rkbgK-l-8L-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
        "\n",
        "for key, col, lab in zip(daily_means,\n",
        "                             [othercol1, othercol2, othercol3, mycol, mycomplcol, othercol4],\n",
        "                             ['Asia', 'Europe', 'Africa', 'N. America', 'S. America', 'Oceania']):\n",
        "    ax.plot(alt_dts_list, daily_means[key], color=col, label = lab)\n",
        "\n",
        "ax.set_xlabel(\"Date\")\n",
        "ax.set_ylabel(\"Reproduction Rate\")\n",
        "ax.set_title(\"Daily Mean Reproduction Rate per Continent\", fontsize=14)\n",
        "tick_marks = np.arange(len(alt_dts_list))\n",
        "ax.set_xticks(tick_marks[::5])\n",
        "ax.set_xticklabels(alt_dts_list[::5], rotation=45)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, title=\"Continent\")\n",
        "plt.show()\n",
        "\n",
        "matplotlib.rc_file_defaults()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.866933Z",
          "iopub.status.idle": "2022-04-10T13:30:59.86721Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.867066Z",
          "shell.execute_reply": "2022-04-10T13:30:59.867081Z"
        },
        "trusted": true,
        "id": "dSFDdsmq8L-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trends and fluctuations that were previously observed can be seen in this graph as well. For example, the overall trend towards higher values of reproduction rate is evident in all continents with the exception of North America. Nonetheless, an additional piece of information provided by this new graph is that during the last days of February the mean reproduction rate for the countries in North America has a tendency to increase. The same can be said for the countries of Asia and South America as well. Especially for the countries in South America, the overall increase of the reproduction rate was approximately 60%, indicating that the overall increase may be even higher by the end of March, based on the aforementioned tendency. On the other hand, when it comes to Oceania, Africa and Europe, there is a tendency towards stabilization of the reproduction rate to a constant value.\n",
        "\n",
        "Closing this part of the analysis, it's worth noting that the intense fluctuations that were observed for Oceania in the .gif above do not seem to appear in this graph, where the corresponding timeseries is close to being constant, if compared to the other ones. However, upon a closer inspection, it becomes clear that there is no inconsistency: the mean reproduction rate is calculated as an unweighted mean value by diving with the total number of countries, instead of a weighted mean with respect to each country's population. This means that the contribution of Australia is considered equal to that of Papua New Guinea for the calculation of the mean reproduction rate. As a result, the almost constant trend of the timeseries corresponding to Oceania can be attributed to the fact that the trends of Australia and Papua New Guinea as far as reproduction rate is concerned are inverse: whenever the rate is high for Australia, it is low for Papua New Guinea and vice versa."
      ],
      "metadata": {
        "id": "E0AsdlAs8L-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Correlation between different features**\n",
        "\n",
        "Moving away from the geographic visualizations we proceed to study the correlation between different features on the country level. More specifically, we study the correlation between excess mortality and number of daily tests performed. The columns used for this purpose are `excess_mortality` and `new_tests_smoothed`. The reason why `new_tests_smoothed` is used instead of `new_tests` is because it contains less missing values compared to `new_tests`. Since the analysis is performed on the country level, there is no reason to use normalized features, since the final metric is simply the correlation between different features corresponding to the same country.\n",
        "\n",
        "It's important to note that not all countries are taken into account, but only these with more than 5 **non zero** entries for the `excess_mortality` feature, for the reasons discussed previously as far as this feature is concerned. Without this filter, a lot of countries seem to show the maximum/minimum value of +1/-1 for the studied correlation, simply because there are very few entries. A typical example is that of Albania, for which the correlation is found equal to +1, simply because it has only 2 entries for the `excess_mortality` feature during the months January-February 2021."
      ],
      "metadata": {
        "id": "t2Yvi6iB8L-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "countries_frame = df.select(\"location\").distinct().collect()\n",
        "exclusion_list = ['Europe', 'World', 'Asia', 'North America', 'South America', 'Africa', 'Oceania', 'Upper middle income']\n",
        "countries_list = [str(countries_frame[x][0]) for x in range(len(countries_frame)) if str(countries_frame[x][0]) not in exclusion_list]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.868092Z",
          "iopub.status.idle": "2022-04-10T13:30:59.868365Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.868223Z",
          "shell.execute_reply": "2022-04-10T13:30:59.868238Z"
        },
        "trusted": true,
        "id": "n3W3AfOT8L-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "country_dict = {}\n",
        "for country in countries_list:\n",
        "    filtered = df.filter(F.col('location') == country).filter(F.col('excess_mortality') != 0.0)\n",
        "    if filtered.count() > 5:\n",
        "        value = filtered.stat.corr(\"excess_mortality\", \"new_tests_smoothed\")\n",
        "    else:\n",
        "        value = np.nan\n",
        "    if not np.isnan(value):\n",
        "        country_dict[country] = value"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.869266Z",
          "iopub.status.idle": "2022-04-10T13:30:59.86959Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.869427Z",
          "shell.execute_reply": "2022-04-10T13:30:59.869449Z"
        },
        "trusted": true,
        "id": "u1C3ya628L-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "country_dict = dict(sorted(country_dict.items(), reverse=True, key=lambda item: item[1]))\n",
        "print(\"As far as the correlation between new tests and excess mortality is concerned:\\n\")\n",
        "print(\"The ten countries with the highest correlation are:\")\n",
        "for i, ct in enumerate(country_dict):\n",
        "    if i == 10: break\n",
        "    print(f\"{ct}, with correlation equal to {country_dict[ct]:.3f}.\")\n",
        "country_dict = dict(sorted(country_dict.items(), reverse=False, key=lambda item: item[1]))\n",
        "print(\"\\nThe ten countries with the lowest correlation are:\")\n",
        "for i, ct in enumerate(country_dict):\n",
        "    if i == 10: break\n",
        "    print(f\"{ct}, with correlation equal to {country_dict[ct]:.3f}.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.870624Z",
          "iopub.status.idle": "2022-04-10T13:30:59.87094Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.87076Z",
          "shell.execute_reply": "2022-04-10T13:30:59.870794Z"
        },
        "trusted": true,
        "id": "I_xrBG4T8L-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the countries where the correlation is positive and close to one, elevated numbers of excess mortality seem to be related to an elevated number of daily tests and vice versa. This might concern countries which, by January or February 2021, had been severely affected by the pandemic and therefore performed a lot of daily tests, in order to be able to restrain the virus outbreak by isolating infected individuals and performing case tracking. [Spain and Portugal](https://pubmed.ncbi.nlm.nih.gov/33334400/) are two examples of such countries.\n",
        "\n",
        "On the other hand, in countries with the inverse correlation, either elevated numbers of excess mortality were not enough to pressure for more diagnostic tests (for example Romania), or despite low numbers of excess mortality, the daily tests performed were highly elevated for prevention (for example New Zealand or Luxemburg)."
      ],
      "metadata": {
        "id": "uxS5d6028L-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving on, it is interesting to also study the correlation between the excess mortality and the course of the vaccination in each country, instead of the daily tests. Note that for this purpose we use the `total_vaccinations` feature instead of the `new_vaccinations` one, since vaccinations are a long-term measure. As a result, their efficiency cannot be imprinted on the number of daily vaccinations. For example, a country in which a high percentage of the population has been vaccinated (for example Israel) is expected to show small numbers of daily vaccinations, without this being an indication of an unvaccinated population."
      ],
      "metadata": {
        "id": "72vLsfv98L-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country_dict = {}\n",
        "for country in countries_list:\n",
        "    filtered = df.filter(F.col('location') == country).filter(F.col('excess_mortality') != 0.0)\n",
        "    if filtered.count() > 5:\n",
        "        value = filtered.stat.corr(\"excess_mortality\", \"total_vaccinations\")\n",
        "    else:\n",
        "        value = np.nan\n",
        "    if not np.isnan(value):\n",
        "        country_dict[country] = value"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.871667Z",
          "iopub.status.idle": "2022-04-10T13:30:59.871978Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.871819Z",
          "shell.execute_reply": "2022-04-10T13:30:59.871837Z"
        },
        "trusted": true,
        "id": "Oj0lL-KO8L-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "country_dict = dict(sorted(country_dict.items(), reverse=True, key=lambda item: item[1]))\n",
        "print(\"As far as the correlation between excess mortality and the course of the vaccinations is concerned:\\n\")\n",
        "print(\"The ten countries with the highest correlation are:\")\n",
        "for i, ct in enumerate(country_dict):\n",
        "    if i == 10: break\n",
        "    print(f\"{ct}, with correlation equal to {country_dict[ct]:.3f}.\")\n",
        "country_dict = dict(sorted(country_dict.items(), reverse=False, key=lambda item: item[1]))\n",
        "print(\"\\nThe ten countries with the lowest correlation are:\")\n",
        "for i, ct in enumerate(country_dict):\n",
        "    if i == 10: break\n",
        "    print(f\"{ct}, with correlation equal to {country_dict[ct]:.3f}.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.873166Z",
          "iopub.status.idle": "2022-04-10T13:30:59.873909Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.873612Z",
          "shell.execute_reply": "2022-04-10T13:30:59.873641Z"
        },
        "trusted": true,
        "id": "uIZ5w1aP8L-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, most countries show a negative correlation (and more specifically close to -1), since the increase in total vaccinations is expected to lead to a reduction in excess mortality, as vaccinations have proven to prevent serious infections from Covid. However, there are still countries such as Cyprus or Croatia, where the correlation is positive. There, it's possible that vaccines became available for the general population during the studied time interval and as a result their efficiency on combating the pandemic has not yet been observed on large scales."
      ],
      "metadata": {
        "id": "AODXlTIz8L-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Covid and general health conditions on the country level**\n",
        "\n",
        "Another interesting aspect of excess mortality is how it correlates with the general health conditions of a country's population. For this reason, we will first calculate the mean value of the 'female_smokers', 'male_smokers', 'diabetes_prevalence' and 'cardiovasc_death_rate' features, using the data on the last available date of our filtered DataFrame. Then, we will sort all countries with respect to their excess mortality per million, since a normalization is required when comparing different countries (and hence different populations). Finally, we will compare the values of the aforementioned features for the top 5 and the bottom 5 countries with their calculated mean values."
      ],
      "metadata": {
        "id": "W-f0A_9x8L-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "this_day = dates_list[-1]\n",
        "filtered_df = df.filter(F.col('date') == this_day)\n",
        "\n",
        "mean_fem_smokers = filtered_df.filter(F.col('female_smokers') != 0.0).select(F.mean(F.col('female_smokers'))).collect()[0][0]\n",
        "mean_male_smokers = filtered_df.filter(F.col('male_smokers') != 0.0).select(F.mean(F.col('male_smokers'))).collect()[0][0]\n",
        "mean_diabetes = filtered_df.filter(F.col('diabetes_prevalence') != 0.0).select(F.mean(F.col('diabetes_prevalence'))).collect()[0][0]\n",
        "mean_card = filtered_df.filter(F.col('cardiovasc_death_rate') != 0.0).select(F.mean(F.col('cardiovasc_death_rate'))).collect()[0][0]\n",
        "\n",
        "print(f'Based on data up to {this_day}, the mean percentage of female smokers is {mean_fem_smokers:.2f}%, while the corresponding number for male smokers is {mean_male_smokers:.2f}%.')\n",
        "print(f'In addition, the mean percentage of people suffering from diabetes (aged 20-79) is {mean_diabetes:.2f}%, while the mean number of deaths per 100.000 people due to cardiovascular conditions is {mean_card:.2f}.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.874963Z",
          "iopub.status.idle": "2022-04-10T13:30:59.87541Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.875168Z",
          "shell.execute_reply": "2022-04-10T13:30:59.875193Z"
        },
        "trusted": true,
        "id": "ylYROcCn8L-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = filtered_df.filter(F.col('diabetes_prevalence') != 0.0).filter(F.col('cardiovasc_death_rate') != 0.0).filter(F.col('female_smokers') != 0.0).filter(F.col('male_smokers') != 0.0)\n",
        "filtered_df.orderBy(\"excess_mortality_cumulative_per_million\", ascending=False).select([\"location\", \"excess_mortality_cumulative_per_million\", \"female_smokers\", \"male_smokers\", \"diabetes_prevalence\", \"cardiovasc_death_rate\"]).toPandas().head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.877114Z",
          "iopub.status.idle": "2022-04-10T13:30:59.877598Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.877325Z",
          "shell.execute_reply": "2022-04-10T13:30:59.87735Z"
        },
        "trusted": true,
        "id": "qJsWCctG8L-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table below depicts the divergence of these characteristics from their mean values for each of the top 5 countries with respect to excess mortality (per million). The red color is used for the entries which are higher than their corresponding mean value.\n",
        "\n",
        "| | Female Smokers (%) | Male Smokers (%) | Diabetic Population (%) | Cardiovascular-related Deaths per 100.000 (%) |\n",
        "| :-: | :-: | :-: | :-: | :-: |\n",
        "| **Armenia** | -85.58 | <font color='red'>59.38</font> | -12.87 | <font color='red'>29.17</font> |\n",
        "| **Mexico** | -33.65 | -34.54 | <font color='red'>60.05</font> | -42.14 |\n",
        "| **Belarus** | <font color='red'>0.96</font> | <font color='red'>41.02</font> | -36.52 | <font color='red'>67.82</font> |\n",
        "| **Russia** | <font color='red'>125</font> | <font color='red'>78.34</font> | -24.26 | <font color='red'>63.34</font> |\n",
        "| **Albania** | -31.73 | <font color='red'>56.62</font> | <font color='red'>23.53</font> | <font color='red'>15.20</font> |\n",
        "\n",
        "First and foremost, it's evident that all of the above countries have at least one of the mentioned features assume a value higher than it's mean. With the exception of Mexico, the countries with the highest excess mortality are characterized by numbers of cardiovascular related deaths that are higher compared to their mean value. The same countries also show significantly increased percentages of male smokers, which is definitely correlated to the number of deaths due to cardiovascular causes. The percentages of female smokers do not show the same tendency, excluding Russia, where the percentage of the divergence from the mean is higher than 100%. Finally, as far as the diabetic population is concerned, Mexico (for which we observed lower numbers with respect to the other features, compared to their mean values) has a value higher than the mean value by 60%."
      ],
      "metadata": {
        "id": "fgfemrQZ8L-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.orderBy(\"excess_mortality_cumulative_per_million\", ascending=True).select([\"location\", \"excess_mortality_cumulative_per_million\", \"female_smokers\", \"male_smokers\", \"diabetes_prevalence\", \"cardiovasc_death_rate\"]).toPandas().head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.878814Z",
          "iopub.status.idle": "2022-04-10T13:30:59.879263Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.879021Z",
          "shell.execute_reply": "2022-04-10T13:30:59.879046Z"
        },
        "trusted": true,
        "id": "CWlpIctY8L-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving on to the bottom 5 countries as far as excess mortality is concerned, it is worth noting that the negative values in these cases is not because of a bug or missing data. On the contrary, it is because the reported numbers of deaths from these countries during the studied period were lower compared to their expected number, based on previous years' reports. A table similar to the one shown for the top 5 can be seen below.\n",
        "\n",
        "| | Female Smokers (%) | Male Smokers (%) | Diabetic Population (%) | Cardiovascular-related Deaths per 100.000 (%) |\n",
        "| :-: | :-: | :-: | :-: | :-: |\n",
        "| **Seychelles** | -31.73 | <font color='red'>9.21</font> | <font color='red'>29.29</font> | -8.11 |\n",
        "| **Barbados** | -81.73 | -55.64 | <font color='red'>66.30</font> | -35.60 |\n",
        "| **Uruguay** | <font color='red'>34.62</font> | -39.13 | -15.07 | -39.14 |\n",
        "| **Mongolia** | -47.12 | <font color='red'>42.25</font> | -40.93 | <font color='red'>74.21</font> |\n",
        "| **Malaysia** | -90.38 | <font color='red'>29.70</font> | <font color='red'>105.15</font> | -1.18 |\n",
        "\n",
        "In this case as well, all countries have at least 1 studied feature with value higher than the corresponding mean. Nonetheless, the most important issue that was present for the top 5 countries, i.e. the elevated numbers of death by cardiovascular causes, does not seem to appear in this case as well, with the exception of Mongolia. When it comes to the percentage of smokers, it appears significantly reduced for both sexes. In contrast to what was observed for the top 5 countries, the countries with the lowest excess mortality tend to have increased diabetic populations.\n",
        "\n",
        "Based on these, one could conclude that the excess mortality due to Covid can be connected to high percentages of smokers in the general population, as well as cardiovascular diseases. Similar conclusions cannot be drawn for the case of diabetes, which may be uncorrelated with deaths due to Covid. Of course, more extensive studies need to be performed in order to draw such conclusions safely, as well as more tests on target groups."
      ],
      "metadata": {
        "id": "glFZxHpV8L-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **k-Means Clustering**\n",
        "\n",
        "Moving on to the final part of this EDA, we incorporate unsupervised learning methods, and more specifically k-Means clustering, in order to draw some additional information from our data. This is our final study on excess mortality and we intend to cluster countries together with respect to it, as well as the total number of cases - both normalized. This clustering will be performed on two different dates: the first and the final date present in our filtered DataFrame, in order to be able to see the evolution of the initial state. As previously done, we will only take into account countries with no missing values (i.e. zeroes) on excess mortality.\n",
        "\n",
        "As is always the case with k-Means clustering, the question that needs to be answered is \"what is the optimal value of k?\". While we could use methods such as the Elbow method to determine a good value for k, the purpose of this notebook is not an extensive study of clustering, but rather the presentation of a few basic methods for EDA using PySpark instead of widely used libraries such as Pandas or scikit-learn, for relatively small datasets. For this reason, we will simply create a scatterplot of the data and determine an optimal value for k through the visualization."
      ],
      "metadata": {
        "id": "LmKrDYmS8L-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "fig, [ax1,ax2] = plt.subplots(1, 2, figsize=(12,5))\n",
        "\n",
        "for idx, (ax,this_day) in enumerate(zip([ax1,ax2],[exc_dates_list[0],exc_dates_list[-1]])):\n",
        "\n",
        "    eff_df = df.filter(F.col('excess_mortality_cumulative_per_million') != 0.0).filter(F.col('date') == this_day).select(['total_cases_per_million','excess_mortality_cumulative_per_million','location'])\n",
        "\n",
        "    pdf = eff_df.select(['total_cases_per_million','excess_mortality_cumulative_per_million']).toPandas()\n",
        "\n",
        "    points = ax.scatter(pdf.total_cases_per_million, pdf.excess_mortality_cumulative_per_million,\n",
        "                                  color=mycol, alpha=0.5)\n",
        "\n",
        "    ax.set_title(f'Scatterplot of Countries as of {this_day}')\n",
        "    ax.set_xlabel('Total Cases per Million')\n",
        "    ax.set_ylabel('Excess Mortality (Cumulative) per Million')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "matplotlib.rc_file_defaults()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.881221Z",
          "iopub.status.idle": "2022-04-10T13:30:59.881684Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.88143Z",
          "shell.execute_reply": "2022-04-10T13:30:59.881454Z"
        },
        "trusted": true,
        "id": "16Y4ukKW8L-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even through this preliminary visualization we can extract a very important conclusion as far as the data themselves are concerned: the number of countries which report on excess mortality has increased by the end of February, since the second scatterplot includes more points.\n",
        "\n",
        "As far as the choice of k is concerned, i.e. the number of clusters to be taken into account, a reasonable hypothesis for the first date is k = 2: one cluster that includes the countries with fewer covid cases per million and one that includes the countries with more covid cases per million, since - with the exception of some outliers - it seems that the excess mortality is proportional to the number of total cases. On the other hand, things are somewhat more complex for the second date. We will be choosing k = 3 for this case, expecting to see the four countries with a very high number of cases (see the second scatterplot) in the same cluster."
      ],
      "metadata": {
        "id": "vR0N7dlO8L-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "numclusters = [2,3]\n",
        "colors = [mycol, mycomplcol, othercol1, othercol2, othercol3, othercol4]\n",
        "\n",
        "fig, [ax1,ax2] = plt.subplots(1, 2, figsize=(14,5))\n",
        "\n",
        "for idx, (ax,this_day) in enumerate(zip([ax1,ax2],[exc_dates_list[0],exc_dates_list[-1]])):\n",
        "\n",
        "    eff_df = df.filter(F.col('excess_mortality_cumulative_per_million') != 0.0).filter(F.col('date') == this_day).filter(F.col('date') == this_day).select(['total_cases_per_million','excess_mortality_cumulative_per_million','location'])\n",
        "\n",
        "    vectorAssembler = VectorAssembler(inputCols = ['total_cases_per_million','excess_mortality_cumulative_per_million'], outputCol = \"features\")\n",
        "    feat_df = vectorAssembler.transform(eff_df)\n",
        "    feat_df = feat_df.select(['features','location'])\n",
        "\n",
        "    kmeans = KMeans().setK(numclusters[idx]).setSeed(1).setFeaturesCol(\"features\").setPredictionCol(\"cluster\")\n",
        "    model = kmeans.fit(feat_df)\n",
        "    transformed = model.transform(feat_df)\n",
        "    centroids = model.clusterCenters()\n",
        "\n",
        "    transformed = transformed.join(eff_df, 'location')\n",
        "\n",
        "    clusters, centers, images = {}, {}, {}\n",
        "\n",
        "    for i in range(numclusters[idx]):\n",
        "\n",
        "        clusters[i] = transformed.filter(F.col('cluster')==i).select(['location','cluster','total_cases_per_million',\n",
        "                                                              'excess_mortality_cumulative_per_million']).toPandas().set_index('location')\n",
        "\n",
        "        images[i] = ax.scatter(clusters[i].total_cases_per_million, clusters[i].excess_mortality_cumulative_per_million,\n",
        "                                  color=colors[i], alpha=0.5)\n",
        "        centers[i] = ax.scatter(centroids[i][0], centroids[i][1], color=colors[i], marker='x')\n",
        "\n",
        "    clusttuple = (images[i] for i in range(numclusters[idx]))\n",
        "    clustnames = ('Cluster '+str(i+1) for i in range(numclusters[idx]))\n",
        "\n",
        "    ax.legend(clusttuple, clustnames, loc='best')\n",
        "\n",
        "    ax.set_title(f'Clusters of Countries as of {this_day}')\n",
        "    ax.set_xlabel('Total Cases per Million')\n",
        "    ax.set_ylabel('Excess Mortality (Cumulative) per Million')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "matplotlib.rc_file_defaults()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.883425Z",
          "iopub.status.idle": "2022-04-10T13:30:59.884011Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.883656Z",
          "shell.execute_reply": "2022-04-10T13:30:59.883681Z"
        },
        "trusted": true,
        "id": "DJrshC1s8L-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In all cases, different clusters are shown with different colors, with each cluster's centroid depicted with an `X`. As far as the first date (early January) is concerned, it is indeed observed that all countries are split into the two clusters in a way that is expected from the initial visualization. As far as the second date (late February) is concerned, it appears that the 4 countries with more than 100.000 total cases per million (> 10%) indeed belong into the same cluster. All of the countries in this cluster are:"
      ],
      "metadata": {
        "id": "6u1Xj4xe8L-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(*clusters[2].index, sep=', ')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.885024Z",
          "iopub.status.idle": "2022-04-10T13:30:59.885313Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.885167Z",
          "shell.execute_reply": "2022-04-10T13:30:59.885182Z"
        },
        "trusted": true,
        "id": "CAwPLtO98L-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As far as the other two clusters are concerned, it appears that the clustering is based solely on the horizontal axis, i.e. the total number of cases per million. In the cluster with the centroid at ~53000 cases per million, 30 out of 39 countries are European countries. On the other hand, in the cluster with the centroid at ~13000 cases per million, there are countries from all over the world. Some of them are European, but under no circumstances do they correspond to the majority, as was the case in the other cluster."
      ],
      "metadata": {
        "id": "AbxTSltV8L-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Closing our investigation on clustering and the project itself, we perform the same steps in order to cluster countries with respect to the virus' reproduction rate and the countries' human development index."
      ],
      "metadata": {
        "id": "eeTnq9sD8L-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "fig, [ax1,ax2] = plt.subplots(1, 2, figsize=(14,5))\n",
        "\n",
        "for idx, (ax,this_day) in enumerate(zip([ax1,ax2],[dates_list[0],dates_list[-1]])):\n",
        "\n",
        "    eff_df = df.filter(F.col('human_development_index') != 0.0).filter(F.col('reproduction_rate') != 0.0).filter(F.col('date') == this_day).select(['human_development_index','reproduction_rate','location'])\n",
        "\n",
        "    pdf = eff_df.select(['human_development_index','reproduction_rate']).toPandas()\n",
        "\n",
        "    points = ax.scatter(pdf.human_development_index, pdf.reproduction_rate, color=mycol, alpha=0.5)\n",
        "\n",
        "    ax.set_title(f'Scatterplot of Countries as of {this_day}')\n",
        "    ax.set_yticks([0.0,0.5,1.0,1.5,2.0])\n",
        "    ax.set_xlabel('Human Development Index')\n",
        "    ax.set_ylabel('Reproduction Rate')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "matplotlib.rc_file_defaults()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.886359Z",
          "iopub.status.idle": "2022-04-10T13:30:59.88702Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.886828Z",
          "shell.execute_reply": "2022-04-10T13:30:59.88685Z"
        },
        "trusted": true,
        "id": "V5Hx63tb8L-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case the number of studied countries seems to be constant in the two cases, unlike what we saw with excess mortality. Furthermore, especially in the final date, there seems to be a homogeneity as far as the reproduction rate is concerned. Before we move on with the clustering, we provide a .gif image (see [Appendix](#appendix) for the code) where the time evolution of the scatterplots is shown for all dates in our filtered DataFrame.\n",
        "\n",
        "<center> <img src=\"https://srigas.me/github/covid_scatterplots.gif\" width=420> </center>\n",
        "\n",
        "As expected, all changes happen only along the vertical axis, since the human development index of a country is not expected to change during such a short period of time. The countries seem to perform an overall oscillatory motion. In addition, during the first days of January there is a sizeable number of countries with a reproduction rate higher than 1.5, however from a point forward the oscillatory motion seems to be limited in the [0.5,1.5] window, in terms of reproduction rate.\n",
        "\n",
        "Moving on to the clustering for the first and the final day of the two-month period, k = 3 is chosen for both cases now, since tests performed using different numbers of k did not result into qualitatively interesting results."
      ],
      "metadata": {
        "id": "M0Do3JAW8L-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "numclusters = [3,3]\n",
        "colors = [mycol, mycomplcol, othercol1, othercol2, othercol3, othercol4]\n",
        "\n",
        "fig, [ax1,ax2] = plt.subplots(1, 2, figsize=(14,5))\n",
        "\n",
        "for idx, (ax,this_day) in enumerate(zip([ax1,ax2],[dates_list[0],dates_list[-1]])):\n",
        "\n",
        "    eff_df = df.filter(F.col('human_development_index') != 0.0).filter(F.col('reproduction_rate') != 0.0).filter(F.col('date') == this_day).select(['human_development_index','reproduction_rate','location'])\n",
        "\n",
        "    vectorAssembler = VectorAssembler(inputCols = ['human_development_index','reproduction_rate'], outputCol = \"features\")\n",
        "    feat_df = vectorAssembler.transform(eff_df)\n",
        "    feat_df = feat_df.select(['features','location'])\n",
        "\n",
        "    kmeans = KMeans().setK(numclusters[idx]).setSeed(1).setFeaturesCol(\"features\").setPredictionCol(\"cluster\")\n",
        "    model = kmeans.fit(feat_df)\n",
        "    transformed = model.transform(feat_df)\n",
        "    centroids = model.clusterCenters()\n",
        "\n",
        "    transformed = transformed.join(eff_df, 'location')\n",
        "\n",
        "    clusters, centers, images = {}, {}, {}\n",
        "\n",
        "    for i in range(numclusters[idx]):\n",
        "\n",
        "        clusters[i] = transformed.filter(F.col('cluster')==i).select(['location','cluster','reproduction_rate',\n",
        "                                                              'human_development_index']).toPandas().set_index('location')\n",
        "\n",
        "        images[i] = ax.scatter(clusters[i].human_development_index, clusters[i].reproduction_rate, color=colors[i], alpha=0.5)\n",
        "        centers[i] = ax.scatter(centroids[i][0], centroids[i][1], color=colors[i], marker='x')\n",
        "\n",
        "    clusttuple = (images[i] for i in range(numclusters[idx]))\n",
        "    clustnames = ('Cluster '+str(i+1) for i in range(numclusters[idx]))\n",
        "\n",
        "    ax.legend(clusttuple, clustnames, loc='best')\n",
        "\n",
        "    ax.set_title(f'Clusters of Countries as of {this_day}')\n",
        "    ax.set_yticks([0.0,0.5,1.0,1.5,2.0])\n",
        "    ax.set_xlabel('Human Development Index')\n",
        "    ax.set_ylabel('Reproduction Rate')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "matplotlib.rc_file_defaults()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-10T13:30:59.890245Z",
          "iopub.status.idle": "2022-04-10T13:30:59.89055Z",
          "shell.execute_reply.started": "2022-04-10T13:30:59.890398Z",
          "shell.execute_reply": "2022-04-10T13:30:59.890415Z"
        },
        "trusted": true,
        "id": "PQ2gWNqy8L-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's obvious that the k-Means clustering is performed with respect to the values of the vertical axis (reproduction rate). In both cases, one of the three clusters includes countries with a relatively small reproduction rate (less than 0.5), while the other two seem to split the countries using R = 1.0 as the decision boundary, as far as the reproduction rate is concerned.\n",
        "\n",
        "As previously stated, this clustering analysis needs to be more carefully performed, perhaps using clustering algorithms with a different principles (for example density-based methods, such as DBSCAN). Unfortunately, PySpark does not yet include such implementations, therefore this task would require the use of scikit-learn in collaboration with Pandas."
      ],
      "metadata": {
        "id": "LHccUlT_8L-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='appendix'></a>\n",
        "\n",
        "## Appendix: Additional Code\n",
        "\n",
        "Below you can find the code used in order to extract the lollipop charts, as well as the scatterplots that were afterwards transformed into .gif files using Adobe Photoshop."
      ],
      "metadata": {
        "id": "8qeF-aGf8L-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mortality Rate\n",
        "\n",
        "\n",
        "```\n",
        "sns.set(style = \"darkgrid\")\n",
        "for this_day in dates_list:\n",
        "    this_day_top_10 = df_for_mort.filter(F.col('date') == this_day).orderBy(\"mortality\", ascending=False).select([\"location\",\"mortality\"]).toPandas()\n",
        "\n",
        "    (markers, stemlines, baseline) = plt.stem(this_day_top_10[0:10]['mortality'])\n",
        "    plt.setp(markers, color=mycol)\n",
        "    plt.setp(stemlines, color=mycol)\n",
        "    plt.setp(baseline, visible=False)\n",
        "    \n",
        "    plt.xticks(range(10), this_day_top_10[0:10]['location'], rotation=45)\n",
        "    plt.title(f'COVID-19 Mortality Rate as of {this_day}', fontsize=14)\n",
        "    plt.savefig(f'lollipop_chart_cases_{this_day}.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "matplotlib.rc_file_defaults()\n",
        "```"
      ],
      "metadata": {
        "id": "VMiyMryR8L-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Total Cases per Million\n",
        "\n",
        "\n",
        "```\n",
        "sns.set(style = \"darkgrid\")\n",
        "for this_day in dates_list:\n",
        "    this_day_top_10 = df.filter(F.col('date') == this_day).orderBy(\"total_cases_per_million\", ascending=False).select([\"location\",\"total_cases_per_million\"]).toPandas()\n",
        "\n",
        "    (markers, stemlines, baseline) = plt.stem(this_day_top_10[0:10]['total_cases_per_million'])\n",
        "    plt.setp(markers, color=mycol)\n",
        "    plt.setp(stemlines, color=mycol)\n",
        "    plt.setp(baseline, visible=False)\n",
        "\n",
        "    plt.xticks(range(10), this_day_top_10[0:10]['location'], rotation=45)\n",
        "    plt.title(f'Total COVID-19 cases per million as of {this_day}', fontsize=14)\n",
        "    plt.savefig(f'lollipop_chart_cases_{this_day}.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "matplotlib.rc_file_defaults()\n",
        "```"
      ],
      "metadata": {
        "tags": [],
        "id": "rM75nQ1Z8L-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Total Deaths per Million\n",
        "\n",
        "```\n",
        "sns.set(style = \"darkgrid\")\n",
        "for this_day in dates_list:\n",
        "    this_day_top_10 = df.filter(F.col('date') == this_day).orderBy(\"total_deaths_per_million\", ascending=False).select([\"location\",\"total_deaths_per_million\"]).toPandas()\n",
        "\n",
        "    (markers, stemlines, baseline) = plt.stem(this_day_top_10[0:10]['total_deaths_per_million'])\n",
        "    plt.setp(markers, color=mycol)\n",
        "    plt.setp(stemlines, color=mycol)\n",
        "    plt.setp(baseline, visible=False)\n",
        "\n",
        "    plt.xticks(range(10), this_day_top_10[0:10]['location'], rotation=45)\n",
        "    plt.title(f'Total COVID-19 deaths per million as of {this_day}', fontsize=14)\n",
        "    plt.savefig(f'lollipop_chart_deaths_{this_day}.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "matplotlib.rc_file_defaults()\n",
        "```"
      ],
      "metadata": {
        "id": "kQgWZVLf8L-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Human Development Index vs Reproduction Rate\n",
        "\n",
        "```\n",
        "sns.set(style = \"darkgrid\")\n",
        "\n",
        "for this_day in dates_list:\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    \n",
        "    eff_df = df.filter(F.col('human_development_index') != 0.0).filter(F.col('reproduction_rate') != 0.0).filter(F.col('date') == this_day).select(['human_development_index','reproduction_rate','location'])\n",
        "    pdf = eff_df.select(['human_development_index','reproduction_rate']).toPandas()\n",
        "    points = ax.scatter(pdf.human_development_index, pdf.reproduction_rate, color=mycol, alpha=0.5)\n",
        "\n",
        "    ax.set_title(f'Scatterplot of Countries as of {this_day}')\n",
        "    ax.set_yticks([0.0,0.5,1.0,1.5,2.0])\n",
        "    ax.set_xlabel('Human Development Index')\n",
        "    ax.set_ylabel('Reproduction Rate')\n",
        "    \n",
        "    plt.savefig(f'hdi_{this_day}.png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "matplotlib.rc_file_defaults()\n",
        "```"
      ],
      "metadata": {
        "id": "zv7bTaNL8L-Y"
      }
    }
  ]
}